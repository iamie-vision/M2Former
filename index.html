<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>M²Former Project Page</title>
  <link rel="icon" type="image/x-icon" href="static/images/iamie.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>

<script>
  window.onload = function() {
    window.scrollTo(0, 0);
  };
</script>
  
<body>
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-2 publication-title">M²Former: Enhancing Event-Based RT-DETR for Robust and Lightweight Space Object Detection</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">Ruitao Pan,</span>
                <span class="author-block">Chenxi Wang<sup>*</sup>,</span>
                  <span class="author-block">Bin Han,</span>
                  <span class="author-block">Xinyu Zhang,</span>
                  <span class="author-block">Zhi Zhai,</span></br>
                  <span class="author-block">Jinxin Liu,</span>
                  <span class="author-block">Naijin Liu,</span>
                  <span class="author-block">Xuefeng Chen, <em>Senior Member, IEEE</em></span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">
                      <a href="https://github.com/iamie-vision" target="_blank">IAMIE, Xi'an Jiaotong University</a><br>
                      Submitted to <em>IEEE Transactions on Geoscience and Remote Sensing</em>
                    </span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Corresponding author (wangchenxi@xjtu.edu.cn)</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                      <!-- PDF link -->
                      <span class="link-block">
                        <a class="external-link button is-normal is-rounded is-light" disabled>
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Pape</span>
                      </a>
                    </span>

                  <!-- Code link -->
                  <span class="link-block">
                    <a href="https://github.com/panruitao/M2Former" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
                      
                <!-- Dataset link -->
                <span class="link-block">
                  <a href="https://zenodo.org/" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-database"></i>
                  </span>
                  <span>Dataset</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser image-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <!-- Your image here -->
      <figure class="has-text-centered">
        <img src="static/images/model_comparison.png" style="width: 95%; height: auto;"/>
      </figure>
      <h2 class="subtitle has-text-centered" style="font-size: 1rem;">
        Performance comparison of YOLO variants, RT-DETR variants, and the proposed M²Former on the E-SPARK dataset, where M²Former* and RT-DETR-R18* denote 
        the variants trained with the area-aware loss and the improved data augmentation strategy. The size of each circle indicates the number of model parameters.
      </h2>
    </div>
  </div>
</section>
<!-- End teaser image -->

  
<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            With increasing human space activities, the detection and tracking of resident space objects (RSOs) has become critical for space situational awareness (SSA). 
            Traditional optical sensors face significant challenges in space environments, such as extreme illumination conditions and motion blur. Event cameras, 
            with their wide dynamic range and high temporal resolution, offer a promising alternative but remain underexplored for spaceborne detection. 
            In this paper, we present the first systematic investigation of event-based space object detection. To address the scarcity of event data, 
            we propose a novel method for generating event streams from static images using controlled affine transformations and advanced event simulators, 
            resulting in a publicly available dataset called <strong>E-SPARK</strong>. Furthermore, we propose a lightweight backbone named <strong>M²Former</strong>for real-time detection, 
            and introduce an area-aware loss (AAL) for small object detection. These components are integrated into the RT-DETR framework. Additionally, we design 
            an improved data augmentation strategy to increase data diversity and supervision density for efficient training. The proposed method is trained and 
            validated on synthetic event data and further evaluated by zero-shot detection on real event data collected from a ground-based testbed. Our method 
            achieves 0.903 AP@50 on synthetic data, outperforming both the baseline RT-DETR-R18 (0.826 AP@50) and the state-of-the-art YOLO variant (0.873 AP@50). 
            In zero-shot detection, our method achieves 0.446 AP@50, surpassing event-based YOLOv8s (0.222 AP@50), event-based RT-DETR-R18 (0.344 AP@50), 
            and image-based YOLOv8s (0.009 AP@50). Moreover, our method reduces the number of parameters and computational complexity by over 50% compared to the baseline, 
            achieving an optimal trade-off between accuracy and efficiency. Experimental results demonstrate the potential of event cameras for robust spaceborne detection
            and the superiority of our method in terms of accuracy, efficiency, and generalization.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      
      <h2 class="title is-3 has-text-centered">Qualitative Results</h2>
      
      <!-- Text description -->
      <div style="max-width: 960px; margin: 0 auto; line-height: 1.8; font-size: 1rem; text-align: justify;">
      <p>
        RepViT demonstrates the success of the MetaFormer paradigm within convolutional networks. Inspired by this, we redesign the RT-DETR backbone into a MetaFormer-style structure. 
        The proposed M²Former enables efficient convolutional and downsampling operations with multi-scale feature extraction capability. It consists of Res2Net, spatial attention, 
        channel attention, and SPD-Conv modules. Detection results on the proposed E-SPARK dataset are shown below.
      </p>
      </div>

      <div id="results-carousel" class="carousel results-carousel">
        <div class="item">
          <!-- Your image here -->
          <img src="static/images/model_architecture.png" style="width: 80%; height: auto; display: block; margin: 0 auto;"/>
          <h2 class="subtitle has-text-centered" style="margin-top: 1rem; font-size: 1rem;">
            Architecture of the proposed M²Former.
          </h2>
        </div>
        <div class="item">
          <!-- Your image here -->
          <img src="static/images/event_generation.png" style="width: 65%; height: auto; display: block; margin: 0 auto;"/>
          <h2 class="subtitle has-text-centered" style="margin-top: 1rem; font-size: 1rem;">
            Comparison of event generation methods: RGB inputs (top), Cao’s method (middle), and the proposed method (bottom). 
          </h2>
        </div>
        <div class="item">
          <!-- Your image here -->
          <img src="static/images/sim_results.png" style="width: 85%; height: auto; display: block; margin: 0 auto;"/>
          <h2 class="subtitle has-text-centered" style="margin-top: 1rem; font-size: 1rem;">
           Detection results of YOLO11m (top), RT-DETR-R18 (middle), and the proposed M²Former (bottom) on the E-SPARK dataset.
          </h2>
        </div>
        <div class="item">
          <!-- Your image here -->
          <img src="static/images/ablation_heatmap.png" style="width: 92%; height: auto; display: block; margin: 0 auto;"/>
          <h2 class="subtitle has-text-centered" style="margin-top: 1rem; font-size: 1rem;">
            Heatmap visualizations of M²Former and its ablated variants on event data. From left to right: RGB reference images, and feature maps from the M²Former, and its ablated versions without Res2Net, spatial attention, channel attention, and SPD-Conv, respectively.
          </h2>
        </div>
      </div>
      
    </div>
  </div>
</section>
<!-- End image carousel -->

  
<!-- Table pdf -->
<section class="hero is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3 has-text-centered">Quantitative Results</h2>
      <div style="max-width: 960px; margin: 0 auto; line-height: 1.8; font-size: 1rem; text-align: justify; margin-bottom: 2rem;">
      <p>
        We adopt standard COCO metrics for object detection benchmarks. <strong>AP@50</strong> represents mean Average Precision at IoU threshold 0.5, which measures overall detection accuracy. <strong>AP@50:95</strong> averages AP over multiple IoU thresholds (0.5 to 0.95 with a step of 0.05), offering a more stringent indicator of localization precision. 
        <strong>APs</strong> denotes average precision for small objects, highlighting performance in challenging small object detecion. In zero-shot detection, we consider it as a category-agnostic task.
      </p>
      <ul style="list-style-type: disc; padding-left: 1.5rem;">
        <li><strong>Table I</strong> lists the augmentation strategies used during training, including Mosaic, Mixup, translation, scaling, and flipping, which enhance data diversity and generalization.</li>
        <li><strong>Table II</strong> compares event representations. Although Event Volume yields slightly better accuracy, Event Histogram is selected for its strong performance and faster preprocessing.</li>
        <li><strong>Table III</strong> shows that M²Former with the AAL and the improved data augmentation strategry achieves the best accuracy, especially for small objects, validating the effectiveness of our design.</li>
        <li><strong>Table IV</strong> highlights that M²Former reduces parameters and GFLOPs by over 50% compared to RT-DETR-R18 while maintaining superior accuracy.</li>
        <li><strong>Table V</strong> presents ablation results of the M²Former, showing the impacts of each module.</li>
        <li><strong>Table VI</strong> confirms that Mosaic and Mixup significantly improve performance, supporting the use of tailored augmentations.</li>
        <li><strong>Table VII</strong> demonstrates that M²Former remains robust even at low input resolution, achieving the highest performance.</li>
        <li><strong>Table VIII</strong> shows that M²Former outperforms other models in zero-shot generalization across lighting conditions.</li>
      </ul>
    </div>
      <iframe src="static/pdf/Tables.pdf" width="100%" height="700"></iframe>
    </div>
  </div>
</section>
<!--End Table pdf -->

  
<!-- Testbed image -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3 has-text-centered">Generalization to Real data</h2>

      <!-- Text description -->
      <div style="max-width: 960px; margin: 0 auto; line-height: 1.8; font-size: 1rem; text-align: justify;">
      <p>
        Zero-shot validation is conducted on real-world data captured from a ground-based testbed, using models trained exclusively on synthetic data without any fine-tuning. We collecte a total of 300 samples using the testbed, covering three illumination conditions: 
        normal exposure, overexposure, and underexposure, with 100 samples per condition. For each sample, RGB images and event data are synchronously recorded by the DAVIS346 camera at the resolution of 346×260. We adopt YOLOv8s as the representative RGB-based detector 
        due to its high detection accuracy and moderate model complexity. 
      </p>
      <p>
        Due to motion blur, color degradation, and texture loss, RGB-based detectors struggle to localize targets under extreme illumination, especially failing completely under underexposure. In contrast, both YOLOv8s and M²Former exhibit stronger robustness when using event data, 
        which is less affected by illumination artifacts. Notably, although M²Former generates more detection boxes than YOLOv8s, these boxes remain consistently close to the target, indicating higher recall and better localization. This highlights a key trade-off: M²Former maintains 
        superior robustness and recall under challenging conditions, whereas YOLOv8s sacrifices recall to preserve precision—often leading to missed detections.
      </p>
      </p>
        In summry, these results underscore the robustness of event data under extreme lighting conditions and demenstrate the strong sim-to-real generalization capability of the M²Former model, making it reliable object detection in real-world space scenarios.
      </p>
      </div>
      
      <!-- Your image -->
      <figure class="has-text-centered" style="margin-top: 2rem;">
        <img src="static/images/testbed.png" style="width: 73%; height: auto;"/>
      </figure>
      <h2 class="subtitle has-text-centered" style="font-size: 1rem;">
        Simulation testbed for real data collection and its hardware components.
      </h2>

      <!-- Youtube vedio -->
      <div style="max-width: 1024px; margin: 0 auto;">
      <!-- Video 1 -->
      <div class="publication-video" style="margin-bottom: 2rem; text-align: center;">
        <iframe src="https://www.youtube.com/embed/Zq2GlADz-Cs?si=xD1isW7ywJqhdHYz" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
      </div>
  
      <!-- Video 2 -->
      <div class="publication-video" style="margin-bottom: 2rem; text-align: center;">
        <iframe src="https://www.youtube.com/embed/bas2jdwTvWI?si=t9pZy-NrZ4PnRMFL" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
      </div>
  
      <!-- Video 3 -->
      <div class="publication-video" style="margin-bottom: 2rem; text-align: center;">
        <iframe src="https://www.youtube.com/embed/uQ4hymHhLUk?si=QFNZAD9vSCzPqpk0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
      </div>
      </div>

      <!-- Image Carousel -->
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item">
          <!-- Your image here -->
          <img src="static/images/real_results_1.png" style="width: 80%; height: auto; display: block; margin: 0 auto;"/>
        </div>
        <div class="item">
          <!-- Your image here -->
          <img src="static/images/real_results_2.png" style="width: 80%; height: auto; display: block; margin: 0 auto;"/>
        </div>
        <div class="item">
          <!-- Your image here -->
          <img src="static/images/real_results_3.png" style="width: 80%; height: auto; display: block; margin: 0 auto;"/>
        </div>
      </div>
      
      <h2 class="subtitle has-text-centered" style="font-size: 1rem; margin-top: 1rem;">
        Detection results under varying illumination conditions. Columns represent normal, overexposed, and underexposed scenes; rows correspond to YOLOv8s (RGB), 
        YOLOv8s (event), RT-DETR-R18 (event), and M²Former (event), respectively. Bounding box colors represent predicted classes, but the task is treated as class-agnostic.
      </h2>
      
    </div>
  </div>
</section>
<!-- End testbed image -->


<!--BibTex citation -->
<hr style="border: none; border-top: 1px solid #ccc; margin-top: 3rem; margin-bottom: 2rem;">
<section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>Coming soon.</code></pre>
<!--       <pre><code>@misc{m2former2025website,
      author       = {Pan, Ruitao and Wang, Chenxi and Han, Bin and others.},
      title        = {M²Former Project Page},
      year         = {2025},
      howpublished = {\url{https://iamie-vision.github.io/M2Former/}},
      note         = {Accessed: 2025-07-21}
}</code></pre> -->
    </div>
</section>
<!--End BibTex citation -->

  
<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
